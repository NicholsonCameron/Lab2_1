{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxJucrKj7mHJ"
   },
   "source": [
    "# Lab 2.1 - Weather Data Around Winona\n",
    "\n",
    "In this lab, we will download and combine a decades worth of weather data from the NOAA, focusing on weather stations within 500 miles of Winona.\n",
    "\n",
    "Here is the outline of the basic process.\n",
    "\n",
    "1. Install and investigate useful packages.\n",
    "2. Find all weather stations in proximity to Winona.\n",
    "3. Use a single station to prototype our tools.\n",
    "4. Automate the process of downloading and uncompressing data from all stations of interest.\n",
    "5. Output the results to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeicFuT-8Vnx"
   },
   "source": [
    "## Problem 1 - Install and investigate useful tools.\n",
    "\n",
    "First, you should install and investigate the following tools.\n",
    "\n",
    "1. **`wget`** is a tool for programmically downloading data files from the web on the command line.  There is a Python wrapper to this tool that you can install with `pip` as shown below.\n",
    "2. **`geopy`** is a package that, among other things, implements a function for computing distances between two lat-long pairs. Again, install this package with `pip` as shown below.\n",
    "3. **`gzip`** is part of the standard Python library and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c8pRv9PyCvPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\gt7194iu\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (3.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uPWEksjS9REC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\users\\gt7194iu\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\gt7194iu\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQXgqoPb9e53"
   },
   "source": [
    "#### Task 1.1 - Investigate using `wget` to download a file.\n",
    "\n",
    "Read the help/documentation on `wget` to figure out how to download the following data file [Some random data file from STAT 210] into the `./data` sub-folder.\n",
    "\n",
    "[https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv](https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'wget' from 'C:\\\\Users\\\\gt7194iu\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\polars\\\\Lib\\\\site-packages\\\\wget.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\gt7194iu\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages\\wget.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Download utility as an easy way to get file from the net\n",
       " \n",
       "  python -m wget <URL>\n",
       "  python wget.py <URL>\n",
       "\n",
       "Downloads: http://pypi.python.org/pypi/wget/\n",
       "Development: http://bitbucket.org/techtonik/python-wget/\n",
       "\n",
       "wget.py is not option compatible with Unix wget utility,\n",
       "to make command line interface intuitive for new people.\n",
       "\n",
       "Public domain by anatoly techtonik <techtonik@gmail.com>\n",
       "Also available under the terms of MIT license\n",
       "Copyright (c) 2010-2015 anatoly techtonik"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D92NkCq_995Z"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "url = \"https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv\"\n",
    "filename = wget.download(url, './Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTW9hZXX-Ceq"
   },
   "source": [
    "#### Task 1.2 - Investigate using `geopy.distance.distance` to compute a distance in miles.\n",
    "\n",
    "1. Import the `distance` function from the `geopy.distance` submodule.\n",
    "2. Use Wikipedia to find the lat-long coordinates of Winona and Rochester MN.\n",
    "3. Use `distance` to compute the distance between Winona and Rochester.\n",
    "4. Use some other source (e.g., Google Maps) to check the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m-XZZQ2v-i4t"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Winona = (44.050556, -91.668333)\n",
    "Rochester = (44.023333, -92.461389)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.54418575388878"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(Winona,Rochester).miles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHdoE7ZFBXlU"
   },
   "source": [
    "#### Task 1.3 - Investigate `gzip`\n",
    "\n",
    "The yearly NOAA data is compressed as `.gz` files, which need to be uncompressed using `gzip`.  Explore the `gzip` module by\n",
    "\n",
    "1. Exploring the documentation/help for the `gzip` module,\n",
    "2. Using `wget` to download the following link into the `./data` folder, and\n",
    "3. Using `gzip` to uncompress this file.\n",
    "4. Inspect the data in your list, which should be of type `byte`.  Use a comprehension with the expression `l.decode('utf-8')` to convert this to a list of strings.\n",
    "5. Write the uncompressed lines to an output file using `with open(path, 'w') as out` and the `writelines` method of `out`.  \n",
    "\n",
    "**Link.** [https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz](https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'gzip' from 'C:\\\\Users\\\\gt7194iu\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\polars\\\\Lib\\\\gzip.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\gt7194iu\\appdata\\local\\anaconda3\\envs\\polars\\lib\\gzip.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Functions that read and write gzipped files.\n",
       "\n",
       "The user of the file doesn't have to worry about the compression,\n",
       "but random access is not allowed."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "88NVuihyCBBO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Data/1750.csv (1).gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz'\n",
    "wget.download(url, './Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'ASN00002061,17500201,PRCP,56,,,a,\\n',\n",
       " b'ASN00003014,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00003059,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00003088,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009015,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009193,17500201,TMIN,187,,,a,\\n',\n",
       " b'ASN00009193,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009500,17500201,DATX,2,,,a,\\n',\n",
       " b'ASN00009500,17500201,MDTX,210,,,a,\\n',\n",
       " b'ASN00009592,17500201,DATX,4,,,a,\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('./Data/1750.csv.gz', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASN00002061,17500201,PRCP,56,,,a,\\n',\n",
       " 'ASN00003014,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00003059,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00003088,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009015,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009193,17500201,TMIN,187,,,a,\\n',\n",
       " 'ASN00009193,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009500,17500201,DATX,2,,,a,\\n',\n",
       " 'ASN00009500,17500201,MDTX,210,,,a,\\n',\n",
       " 'ASN00009592,17500201,DATX,4,,,a,\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = [l.decode('utf-8') for l in lines]\n",
    "output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/1750.csv', 'w') as f:\n",
    "    out = f.writelines(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt1MlIz9DWpm"
   },
   "source": [
    "## Problem 2 - Find all stations within 500 miles of Winona, MN.\n",
    "\n",
    "The file linked below contains information about all stations tracked by NOAA.  \n",
    "\n",
    "*Main folder:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/\n",
    "\n",
    "*Station txt file:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n",
    "\n",
    "*Note.* While it would be easier to use the CSV version of the station file, you should use the TXT version here (for practice).\n",
    "\n",
    "**Your tasks** Our goal is to get a list of stations that are within 25 miles of Winona.  Do this by\n",
    "\n",
    "1. Using `wget` to download the stations information into the `./data` folder.\n",
    "2. Use `with` to read the lines of this file.\n",
    "3. At this point, the lines are strings in a fixed-width format separated by whitespace.  Use a list comprehension with the string split method to split the raw lines (strings) into a list of entries.\n",
    "4. There are three entries of interest, the station ID and the lat-long coordinates of the station.  Inspect the file to determine the index for these three entries.\n",
    "5. We want to transform the lines (currently a list of strings) into a record, which is a `dict` with good names for the entries as keys and the values representing the data in an appropriate type (string for station ID, `float` for the lat-long).  Use a comprehension to create a list of records as described.\n",
    "6. Use another comprehension to apply a filter to the stations, keeping only those within 25 miles of Winona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F0J443KoFTrs"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Your code here (add cells as needed)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mwget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\site-packages\\wget.py:526\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, out, bar)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     binurl \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m--> 526\u001b[0m (tmpfile, headers) \u001b[38;5;241m=\u001b[39m \u001b[43mulib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m filename \u001b[38;5;241m=\u001b[39m detect_filename(url, out, headers)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outdir:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\urllib\\request.py:240\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    241\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\polars\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "# Your code here (add cells as needed)\n",
    "url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "wget.download(url, './Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       \\n',\n",
       " 'ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    \\n',\n",
       " 'AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\\n',\n",
       " 'AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194\\n',\n",
       " 'AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217\\n',\n",
       " 'AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218\\n',\n",
       " 'AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930\\n',\n",
       " 'AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938\\n',\n",
       " 'AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948\\n',\n",
       " 'AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Data/ghcnd-stations.txt') as f:\n",
    "    lines = f.readlines()\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ACW00011604',\n",
       "  '17.1167',\n",
       "  '-61.7833',\n",
       "  '10.1',\n",
       "  'ST',\n",
       "  'JOHNS',\n",
       "  'COOLIDGE',\n",
       "  'FLD'],\n",
       " ['ACW00011647', '17.1333', '-61.7833', '19.2', 'ST', 'JOHNS']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lines = [line.split() for line in lines]\n",
    "split_lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\n",
    "    'Station ID',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Station ID': 'ACW00011604', 'latitude': '17.1167', 'longitude': '-61.7833'},\n",
       " {'Station ID': 'ACW00011647', 'latitude': '17.1333', 'longitude': '-61.7833'},\n",
       " {'Station ID': 'AE000041196', 'latitude': '25.3330', 'longitude': '55.5170'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_rows = [\n",
    "    {col_name: row[id] for id, col_name in enumerate(header)}\n",
    "    for row in split_lines\n",
    "]\n",
    "dict_rows[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Station ID': 'US1MNHS0001', 'latitude': '43.8350', 'longitude': '-91.3140'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radius = 25\n",
    "\n",
    "nearby_stations = [\n",
    "    station for station in dict_rows\n",
    "    if distance(Winona, (float(station['latitude']), float(station['longitude']))).miles <= radius]\n",
    "\n",
    "nearby_stations[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7_9ve26IduT"
   },
   "source": [
    "#### Problem 3 - Prototype downloading and uncompressing a station file.\n",
    "\n",
    "Before we download and uncompress all the stations of interest, let's practice on one station file.\n",
    "\n",
    "\n",
    "1. Copy the url for some station and store is as a variable named `url`.\n",
    "2. Write `lambda` functions that extract each of the following from the station `url`: compressed file name, compressed file path (e.g., `./data/...`), and uncompressed file path (e.g., `./data/...`).\n",
    "3. Write a `lambda` function that extracts\n",
    "4. Use `wget` to download this stations data.\n",
    "5. Use `gzip` to uncompress the data.\n",
    "6. Write the data to out output file.\n",
    "\n",
    "Your code should have the following shape:\n",
    "\n",
    "```{Python}\n",
    "wget.download(...)\n",
    "with gzip.open(...) as f:\n",
    "    with open(..., 'w') as out:\n",
    "        f.readlines()\n",
    "        out.writelines(f)\n",
    "```\n",
    "\n",
    "You should be using your helper functions to, in part, fill in the `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "04YS60A1JciS"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0001.csv.gz'\n",
    "\n",
    "headers = [\n",
    "    'ID',\n",
    "    'Year/Month/Day',\n",
    "    'Element',\n",
    "    'Data Value',\n",
    "    'M-Flag',\n",
    "    'Q-Flag',\n",
    "    'S-Flag',\n",
    "    'OBS-Time'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_filename = lambda url: url.split(\"/\")[-1] \n",
    "get_compressed_path = lambda url: f\"./Data/{get_filename(url)}\"\n",
    "get_uncompressed_path = lambda url: f\"./Data/{get_filename(url).replace('.gz', '')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_file = get_compressed_path(url)\n",
    "uncompressed_file = get_uncompressed_path(url)\n",
    "\n",
    "wget.download(url, compressed_file)\n",
    "\n",
    "with gzip.open(compressed_file, 'rt', encoding='utf-8') as f:\n",
    "    with open(uncompressed_file, 'w', encoding='utf-8') as out:\n",
    "        out.write(','.join(headers) + '\\n')\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            columns = line.split()\n",
    "            out.write(','.join(columns) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzgFqv5VF38i"
   },
   "source": [
    "## Problem 4 - Build the station URLs and download the files.\n",
    "\n",
    "**Tasks.** Now you need to build urls for all stations of interest by\n",
    "\n",
    "1. Use a comprehension to extract the stations of interest into a list.\n",
    "2. Investigating the structure of the files stored in the `by_station` folder (see main folder link above).\n",
    "3. Use a comprehension and an `f` string to build a list of URLS for all stations of interest.\n",
    "4. Use `wget` to download the data for the stations of interest into the data folder.\n",
    "5. Use `gzip` to uncompress the files.\n",
    "6. Convert the `bytes` to `str` of format `utf-8`.\n",
    "7. Use the append mode `\"a\"` of `open` with `writelines` to append the data in each file to your output file.\n",
    "\n",
    "While we usually avoid using a `for` loop, we make an exception for code for lengthy IO.  To accomplish steps 4 & 5, use a `for` loop with the following shape.\n",
    "\n",
    "```{Python}\n",
    "for url in station_urls:\n",
    "    wget.download(...)\n",
    "    with gzip.open(...) as f:\n",
    "        with open(..., 'a') as out:\n",
    "            f.readlines()\n",
    "            ... # Convert lines to strings here\n",
    "            out.writelines(f)\n",
    "    print(f\"Downloaded and extracted the data for {url}\")\n",
    "```\n",
    "\n",
    "Note that the code inside the loop should resemble the code from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SOUt7rCBIZ6f"
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "nearby_stations = [\n",
    "    station for station in dict_rows\n",
    "    if distance(Winona, (float(station['latitude']), float(station['longitude']))).miles <= radius]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_urls = [\n",
    "    f\"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/{station['Station ID']}.csv.gz\"\n",
    "    for station in nearby_stations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tnGWOC1xF9kT",
    "outputId": "9b060cfe-aa8e-4930-880c-b720a36fc911"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://my_fake_website.cool/A123456789'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_station = \"A123456789\"\n",
    "make_fake_url = lambda s: f\"https://my_fake_website.cool/{s}\"\n",
    "\n",
    "make_fake_url(fake_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RPvTJEJGYtc",
    "outputId": "da643b74-66a9-4b29-8cce-a679a2d9fa5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://my_fake_website.cool/A0',\n",
       " 'https://my_fake_website.cool/A1',\n",
       " 'https://my_fake_website.cool/A2',\n",
       " 'https://my_fake_website.cool/A3',\n",
       " 'https://my_fake_website.cool/A4',\n",
       " 'https://my_fake_website.cool/A5',\n",
       " 'https://my_fake_website.cool/A6',\n",
       " 'https://my_fake_website.cool/A7',\n",
       " 'https://my_fake_website.cool/A8',\n",
       " 'https://my_fake_website.cool/A9']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_fake_stations =[f'A{i}' for i in range(10)]\n",
    "\n",
    "(my_fake_urls := [make_fake_url(s) for s in my_fake_stations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jVG4xpnvMvRK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0001.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0006.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0007.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0008.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0009.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0012.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0013.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0022.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0023.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNOL0038.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNOL0991.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWB0015.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0004.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0005.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0006.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0007.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0009.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0011.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0013.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0019.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0021.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0023.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0024.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0026.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0027.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0029.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0031.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0033.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WIBF0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0010.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0011.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0018.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0022.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0004.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0005.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0008.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0010.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00210146.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00210559.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00213812.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00214418.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00215488.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00217184.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00217277.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00218951.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219067.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219072.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219077.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00470124.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472165.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472992.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472996.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00474366.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00478589.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USW00004956.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USW00014920.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "output_file = './Data/all_station_data.csv'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    out.write(','.join(headers) + '\\n')\n",
    "\n",
    "for url in station_urls:\n",
    "    compressed_file = f\"./data/{url.split('/')[-1]}\"\n",
    "    wget.download(url, compressed_file)\n",
    "\n",
    "    with gzip.open(compressed_file, 'rt', encoding='utf-8') as f:\n",
    "        with open(output_file, 'a', encoding='utf-8') as out:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    out.write(','.join(line.split('\\t')) + '\\n')\n",
    "    \n",
    "    print(f\"Downloaded and extracted the data for {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
